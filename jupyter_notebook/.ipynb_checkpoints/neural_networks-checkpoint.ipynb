{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5709dd2f-569d-40ff-8d30-d0d815498ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# parameters initialization\n",
    "def initialize_parameters(input_dim, hidden_dim, output_dim):\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(2 / (input_dim + hidden_dim))\n",
    "    b1 = np.zeros((1, hidden_dim))\n",
    "    W2 = np.random.randn(hidden_dim, output_dim) / np.sqrt(2 / (hidden_dim + output_dim))\n",
    "    b2 = np.zeros((1, output_dim))\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3e120-6fcd-4bca-a183-ffa43a6d16ad",
   "metadata": {},
   "source": [
    "# Parameters Initialization\n",
    "`num_examples`: the number of examples\n",
    "\n",
    "`input_dim`: the number of neurons in the input layer\n",
    "\n",
    "`hidden_dim`: the number of neurons in the hidden layer\n",
    "\n",
    "`output_dim`: the number of neurons in the softmax layer/the number of categories\n",
    "\n",
    "$\\textbf{X}$: `num_examples` * `input_dim` **Matrix**\n",
    "\n",
    "$\\textbf{W}_1$: `input_dim` * `hidden_dim` **Matrix**\n",
    "\n",
    "$\\vec{b}_1$: `1` * `hidden_dim` **Row Vector**\n",
    "\n",
    "$\\textbf{W}_2$: `hidden_dim` * `output_dim` **Matrix**\n",
    "\n",
    "$\\vec{b}_2$: `1` * `output_dim` **Row Vector**\n",
    "\n",
    "> *Notice:*\n",
    ">\n",
    "> In this part, the method $Xavier \\ Initialization$ is adopted to initialize the weights.\n",
    ">\n",
    "> The weight matrix will be initialized as:\n",
    ">\n",
    "> $W \\sim \\mathcal{N}(0, \\frac{2}{n + m})$\n",
    "> * n: the number of neurons in previous layer\n",
    "> * m: the number of neurons in current layer\n",
    "> * Sampling from a normal distribution with mean $0$ and variance $\\frac{2}{n+m}$\n",
    ">\n",
    "> By using Xavier initialization, the problem of vanishing or exploding gradients can be avoided by effectively keeping the variance of the inputs and outputs the same. This method has been shown in practice to accelerate model convergence and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d294a09-a975-4079-951c-249384e17f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    return probs, a1, W2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea250db3-4cd5-4e8c-866e-31b90a82421d",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "`num_examples` * `hidden_dim` input of the hidden layer: $\\textbf{z}_1 = \\textbf{X} \\cdot \\textbf{W}_1 + \\vec{b}_1$\n",
    "\n",
    "`num_examples` * `hidden_dim`output of the hidden layer: $\\textbf{a}_1 = \\tanh{\\textbf{z}_1}$\n",
    "\n",
    "`num_examples` * `output_dim` input of the softmax layer: $\\textbf{z}_2 = \\textbf{a}_1 \\cdot \\textbf{W}_2 + \\vec{b}_2$\n",
    "\n",
    "`num_examples` * `output_dim` output of the softmax layer: $\\hat{\\textbf{y}} = \\textbf{probs} = softmax(\\textbf{z}_2)$\n",
    "\n",
    "> *Notice:*\n",
    "> \n",
    "> `NumPy` features a broadcast mechanism that allows arrays of different shapes to be compatible in arithmetic operations. In this case, $\\vec{b}_1$ is automatically expanded to `num_examples` * `hidden_dim` size matrix, and $\\vec{b}_2$ is automatically expanded to `num_examples` * `output_dim` size matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "005628aa-5fb6-4e35-bfad-9299dcb61af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back propagation\n",
    "def back_propagation(X, y, probs, a1, W2):\n",
    "    delta3 = probs - y\n",
    "    dW2 = (a1.T).dot(delta3)\n",
    "    db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "    delta2 = (1 - np.power(a1, 2)) * delta3.dot(W2.T)\n",
    "    dW1 = (X.T).dot(delta2)\n",
    "    db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21044a79-2d80-48e8-b8aa-3b4ee7ccf912",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "* Standard cross entropy loss function:\n",
    "  \n",
    "  $L = -\\sum_j y_j \\log{\\hat{y}}_j$\n",
    "\n",
    "* Let's say there is only 1 example input, and let $z_i$ denote the $i$-th element of the input vector of the softmax layer $\\vec{z}$, $\\hat{y}_i$ denote the $i$-th element of the output vector $\\vec{\\hat{y}}$, and the derivation of cross entropy loss is given as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979493d9-37b8-41fc-b2b6-aa430c23e094",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial z_i}\n",
    "& = - \\sum_k y_k \\cdot \\frac{\\partial \\log{\\hat{y}_j}}{\\partial z_i} \\\\\n",
    "& = -\\sum_k y_k \\cdot \\frac{1}{\\hat{y}_k} \\cdot \\frac{\\partial{\\hat{y}_k}}{\\partial z_i} \\\\\n",
    "& = -y_i(1-\\hat{y}_i)-\\sum_{k \\neq i} y_k \\cdot \\frac{1}{\\hat{y}_k} \\cdot \\frac{\\partial{\\hat{y}_k}}{\\partial z_i} \\\\\n",
    "& = -y_i + y_i \\hat{y}_i + \\sum_{k \\neq i}{y_k \\hat{y}_i} \\\\\n",
    "& = -y_i + \\hat{y}_i \\sum_k y_k \\\\\n",
    "& = \\hat{y}_i - y_i \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4348c0-b19a-4f10-bd16-47f94c8ca821",
   "metadata": {},
   "source": [
    "* thus we can calculate the gradient descent of parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8680c-7ed2-4420-92b7-bc1e26fd2755",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial{L}}{\\partial{b}_2} &= \\frac{\\partial{L}}{\\partial{z}_2} \\cdot \\frac{\\partial{z}_2}{\\partial{b}_2} = \\delta_3 \\\\\n",
    "\\frac{\\partial{L}}{\\partial{W}_2} &= \\frac{\\partial{L}}{\\partial{z}_2} \\cdot \\frac{\\partial{z}_2}{\\partial{W}_2} = a^{T}_1 \\cdot \\delta_3 \\\\\n",
    "\\frac{\\partial{L}}{\\partial{b}_1} &= \\frac{\\partial{L}}{\\partial{z}_2} \\cdot \\frac{\\partial{z}_2}{\\partial{a}_1} \\cdot \\frac{\\partial{a}_1}{\\partial{z}_1} \\cdot \\frac{\\partial{z}_1}{\\partial{b}_1} = (1 - a^2_1) * (\\delta_3 \\cdot W^T_2) = \\delta_2 \\\\\n",
    "\\frac{\\partial{L}}{\\partial{W}_1} &= \\frac{\\partial{L}}{\\partial{z}_2} \\cdot \\frac{\\partial{z}_2}{\\partial{a}_1} \\cdot \\frac{\\partial{a}_1}{\\partial{z}_1} \\cdot \\frac{\\partial{z}_1}{\\partial{W}_1} = X^T \\cdot \\delta_2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7ae0f-586d-45bd-9df0-89ee3bedb002",
   "metadata": {},
   "source": [
    "* $\\delta_3$ is the `num_examples` * `output_dim` size error matrix of the softmax layer, and $\\delta_2$ is the `num_examples` * `hidden_dim` size error matrix of the hidden layer.\n",
    "\n",
    "    `np.sum(delta2, axis=0)` sums up each column of $\\delta_2$ and get a `1` * `hidden_dim` size row vector,      `np.sum(delat3, axis=0, keepdims=True)` sums up each column of $\\delta_3$ and get a `1` * `output_dim` size row vector. Each element of these vectors represents the total error of the corresponding neuron over all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d4c2e-cf6d-4b1a-b839-1003d7791041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edward",
   "language": "python",
   "name": "edward"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
